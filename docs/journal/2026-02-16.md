# Journal — 2026-02-16

## Extracted `parse_object_header` from `read_object`

My `read_object` function in `src/core/object.c` was doing two things:
reading/decompressing a git object from disk and parsing its header
(`type SP size NUL`). I separated the header parsing into a standalone utility
function for better separation of concerns.

### What I changed

- **New function `parse_object_header`** (`src/core/utils.c`) — Pure parsing
  logic I extracted from `read_object`. Takes a raw buffer and its length,
  returns the parsed type string, content size, and payload offset through
  output pointers. No allocations, no I/O — just parsing with validation.
- **Simplified `read_object`** (`src/core/object.c`) — Now delegates header
  parsing to `parse_object_header` and only handles orchestration: file I/O,
  decompression, size-mismatch validation, and populating the `git_object_t`
  struct. I use `strdup` for the type instead of manual malloc + memcpy.
- **Prototype in `core.h`** — I added the declaration with corrected signature:
  `const unsigned char *buf` (matches `buffer_t.data`), `buf_len` for safe
  bounds checking, and pointer parameters (`size_t *`) for the output values.
- **New constant `CGIT_MAX_TYPE_LEN`** (`common.h`) — Replaces the magic
  number `32` I had for the type buffer size. Set to 16, which covers all
  standard git object types (blob, tree, commit, tag).

### Why this split

`parse_object_header` is a pure utility that I can reuse in `write_object`
or any future code that needs to inspect object headers without reading them
from disk. Keeping it allocation-free makes it easy to test and compose.

## Added idempotency check to `write_object`

My `write_object` was unconditionally writing the compressed object to disk,
even if the same object already existed. Since the hash determines the path, the
content is identical by definition — but the redundant mkdir, compress, and
fwrite are wasted I/O.

### What I changed

- **New `stat` guard** (`src/core/object.c`) — After building the object path,
  a `stat()` call checks whether the file already exists. If it does, the
  function skips directory creation, compression, and writing, and jumps
  straight to cleanup. The hash is still computed and returned via `hash_out`
  regardless.

### Why

This matches real git's behavior: object writes are idempotent and skip the
disk write when the object is already stored. It avoids unnecessary I/O and
prevents potential issues under concurrent writes.

## Replaced hardcoded macro values with derived expressions

`CGIT_DIR_BUF_SIZE` and `CGIT_OBJ_NAME_BUF_SIZE` in `common.h` were defined
as magic numbers (`3` and `39`). I replaced them with expressions that make the
intent explicit.

### What I changed

- **`CGIT_DIR_BUF_SIZE`** — Changed from `3` to `(2 + 1)`: 2 hex characters
  for the directory name plus the null terminator.
- **`CGIT_OBJ_NAME_BUF_SIZE`** — Changed from `39` to
  `(CGIT_HASH_HEX_LEN - 2 + 1)`: the remaining hex characters after the
  2-character directory prefix, plus the null terminator. Now derived from
  `CGIT_HASH_HEX_LEN` instead of being independently hardcoded.

### Why

Hardcoded values obscure the relationship between the macros. If
`CGIT_HASH_HEX_LEN` ever changed, `CGIT_OBJ_NAME_BUF_SIZE` would silently be
wrong. Deriving it from the source constant keeps the definitions consistent by
construction.

## Added early returns to `is_valid_hash`

The function was accumulating its result into a `cgit_error_t result` variable
and returning it at the end, even though each validation check is independent
and final — if the length is wrong, there's no point checking individual
characters.

### What I changed

- **Removed the `result` variable** (`src/core/utils.c`) — Each validation
  failure now returns `CGIT_ERROR_INVALID_ARGS` immediately instead of
  assigning to `result` and falling through.
- **Explicit `return CGIT_OK`** at the end — Only reached when all checks pass.

### Why

Early returns avoid unnecessary computation (no point iterating 40 characters
if the length is already wrong) and make the control flow easier to follow.
Since `is_valid_hash` owns no resources that need cleanup, early returns are
safe and simpler than the `goto cleanup` pattern.
